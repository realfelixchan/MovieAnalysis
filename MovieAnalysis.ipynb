{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "96d3c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3154a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save a json file with api keys for tmdb, openai, pinecone, and postgres username/password\n",
    "api_keys = json.load(open(\"./Credentials/api_keys.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dfcb49",
   "metadata": {},
   "source": [
    "## Misc functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92d61391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "def closest_key(query, dictionary):\n",
    "    closest_key = min(dictionary.keys(), key=lambda k: Levenshtein.distance(query, k))\n",
    "    return dictionary[closest_key]\n",
    "def find_repeated_first_elements(tuples_list):\n",
    "    element_dict = defaultdict(list)\n",
    "    repeated_elements = {}\n",
    "    for t in tuples_list:\n",
    "        element_dict[t[0]].append(t)\n",
    "    \n",
    "    for key, value in element_dict.items():\n",
    "        if len(value) > 1:\n",
    "            repeated_elements[key] = value\n",
    "    \n",
    "    return repeated_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252d60ea",
   "metadata": {},
   "source": [
    "# First Functionality - semantic search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc3730",
   "metadata": {},
   "source": [
    "## Scrape from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a47e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We get a list of all movies from wikipedia which serves as a source of information that will be utilized in the semantic\n",
    "#search aspect of this tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad84a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_urls = [\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_A\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_B\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_C\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_D\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_E\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_F\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_G\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_H\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_I\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_J%E2%80%93K\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_L\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_M\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_N%E2%80%93O\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_P\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_Q%E2%80%93R\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_S\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_T\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_U%E2%80%93W\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_X%E2%80%93Z\"\n",
    "]\n",
    "def get_soup(url):\n",
    "    response = requests.get(url)\n",
    "    page_content = response.content\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    return soup\n",
    "def get_all_movies(url):\n",
    "    soup = get_soup(url)\n",
    "    movie_info = []\n",
    "\n",
    "    for li in soup.find_all('li'):\n",
    "        a = li.find('a')\n",
    "        if a:\n",
    "            title = a.get('title')\n",
    "            href = a.get('href')\n",
    "\n",
    "            year_match = re.search(r'\\((\\d{4})\\)', li.text)\n",
    "            year = year_match.group(1) if year_match else 'Unknown'\n",
    "            if year == \"Unknown\":\n",
    "                continue\n",
    "            try:\n",
    "                title = re.sub(r' \\([^)]*(film|movie)[^)]*\\)', '', title)\n",
    "            except:\n",
    "                continue\n",
    "            movie_info.append((title, href, year))\n",
    "    \n",
    "    for li in soup.find_all('li'):\n",
    "        main_title = li.find('i').text if li.find('i') else ''\n",
    "\n",
    "        a_tags = li.find_all('a')\n",
    "        for a in a_tags:\n",
    "            title = a.get('title', main_title)\n",
    "            href = a.get('href', '')\n",
    "\n",
    "            year_match = re.search(r'(\\d{4})', a.text)\n",
    "            year = year_match.group(1) if year_match else 'Unknown'\n",
    "            if year == \"Unknown\":\n",
    "                continue\n",
    "            title = re.sub(r' \\([^)]*(film|movie)[^)]*\\)', '', title)\n",
    "            movie_info.append((title, href, year))\n",
    "    return list(set(movie_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddbd96c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_movies = []\n",
    "for url in all_urls:\n",
    "    all_movies = all_movies + get_all_movies(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdff55c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(all_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d44b5aa-8610-47b4-af88-05459cc70595",
   "metadata": {},
   "source": [
    "#### In all_movies, there's an entry  ('Program (The Animatrix)', '/wiki/Program_(The_Animatrix)', '2003')], but there's no actual movie by that name. Program is an episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "686ba120",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_movies = get_all_movies(\"https://en.wikipedia.org/wiki/List_of_films:_D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbaa3ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_subset = d_movies[800:850]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ec15eb",
   "metadata": {},
   "source": [
    "## Obtaining snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c8f8047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We scrape the particular wikipedia pages and scrape a \"snippet\" chunk of text to reference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "215fcede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    if overlap >= n:\n",
    "        return \"Overlap must be smaller than chunk size\"\n",
    "    length = len(text)\n",
    "    start = 0\n",
    "    chunks = []\n",
    "    while start < length:\n",
    "        end = min(start + n, length)\n",
    "        chunks.append(text[start:end])\n",
    "        start += n - overlap\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "def get_page_content(href):\n",
    "    soup = get_soup(f'https://en.wikipedia.org/{href}')\n",
    "    article_container = soup.find('div', {'id': 'mw-content-text'})\n",
    "    article_text = ''\n",
    "    for paragraph in article_container.find_all('p'):\n",
    "        article_text += paragraph.text\n",
    "    infobox = soup.find('table', {'class': 'infobox'})\n",
    "\n",
    "    infobox_text = \"\"\n",
    "\n",
    "    if infobox:\n",
    "        for row in infobox.find_all('tr'):\n",
    "            # Extract the header if available\n",
    "            th = row.find('th')\n",
    "            if th:\n",
    "                infobox_text += th.get_text() + \"\\n\"\n",
    "\n",
    "            # Extract the data in the row\n",
    "            td = row.find('td')\n",
    "            if td:\n",
    "                infobox_text += td.get_text() + \"\\n\"\n",
    "    article_text += infobox_text\n",
    "    div_tag = soup.find('div', {'class': 'div-col'})\n",
    "    ul_tag = div_tag.find('ul') if div_tag else None\n",
    "    list_text = \"\"\n",
    "    if ul_tag:\n",
    "        for li_tag in ul_tag.find_all('li'):\n",
    "            list_text += li_tag.get_text()\n",
    "    article_text += list_text\n",
    "    return article_text\n",
    "\n",
    "def get_chunked_embeddings(href, embedding_model):\n",
    "    page_content = get_page_content(href)\n",
    "    embeddings = embedding_model.encode(chunker(page_content))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4cee8a",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "574a3f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding models allow us to encode our sentences into vectors, particularly we will be using a combination of\n",
    "#sparse and dense embedding models for fine grained semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a92e9ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pleasework\\PycharmProjects\\MovieAnalysis\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util\n",
    "from splade.models.transformer_rep import Splade\n",
    "import torch\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12f4c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "478b4e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8859]])\n",
      "tensor([[0.8596]])\n"
     ]
    }
   ],
   "source": [
    "#intutive embedding test\n",
    "test_model = SentenceTransformer('all-mpnet-base-v2',device=device)\n",
    "query_embedding = test_model.encode(\"thriller, crime\")\n",
    "passage_embedding = test_model.encode(\"thriller, drama\")\n",
    "passage_embedding2 = test_model.encode(\"thriller, comedy\")\n",
    "print(util.dot_score(query_embedding, passage_embedding))\n",
    "print(util.dot_score(query_embedding, passage_embedding2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a42272b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_model = SentenceTransformer(\n",
    "    'msmarco-bert-base-dot-v5',\n",
    "    device=device\n",
    ")\n",
    "dense_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12660d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_model_id = 'naver/splade-cocondenser-ensembledistil'\n",
    "\n",
    "sparse_model = Splade(sparse_model_id, agg='max')\n",
    "sparse_model.to(device)\n",
    "sparse_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5015d62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(sparse_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7fe0be",
   "metadata": {},
   "source": [
    "## Scrape TMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5284718",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This was the first site that I found that gave metadata information for movies, letterbox may be more comprehensive\n",
    "#but this information is suitable for this first functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb650654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tmdbsimple as tmdb\n",
    "tmdb.API_KEY = api_keys[\"TMDB_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d8bd116",
   "metadata": {},
   "outputs": [],
   "source": [
    "#genre codes tmdb uses\n",
    "movie_genre_dict = {28:\"Action\",12:\"Adventure\",16:\"Animation\", 35:\"Comedy\", 80:\"Crime\", 99:\"Documentary\",18:\"Drama\", 10751:\"Family\",14:\"Fantasy\",36:\"History\",27:\"Horror\",10402:\"Music\",9648:\"Mystery\",10749:\"Romance\",878:\"Science Fiction\",10770:\"TV Movie\", 53:\"Thriller\", 10752:\"War\",37:\"Western\"}\n",
    "def id2name(id_list):\n",
    "    return [movie_genre_dict[idz] for idz in id_list]\n",
    "#language codes tmdb uses\n",
    "language_codes = {\"af\":\"Afrikaans\", \"sq\":\"Albanian\", \"ar\":\"Arabic\", \"eu\": \"Basque\", \"bg\":\"Bulgarian\",\n",
    "                  \"ca\":\"Catalan\", \"cn\":\"Chinese\", \"zh\":\"Chinese\", \"hr\":\"Croatian\", \"cs\":\"Czech\", \"da\": \"Danish\", \"nl\":\"Dutch\",\n",
    "                  \"en\":\"English\", \"et\": \"Estonian\", \"fi\":\"Finnish\", \"fr\":\"French\", \"de\": \"German\", \"el\": \"Greek\",\n",
    "                  \"he\":\"Hebrew\", \"hi\": \"Hindi\", \"hu\":\"Hungarian\", \"is\":\"Icelandic\", \"in\": \"Indonesian\", \"it\": \"Italian\",\n",
    "                  \"ja\": \"Japanese\", \"ko\": \"Korean\", \"lv\": \"Latvian\", \"lt\":\"Lithuanian\", \"mk\": \"Macedonian\", \"ms\": \"malay\",\n",
    "                  \"no\": \"Norwegian\", \"pl\":\"Polish\", \"pt\": \"Portugese\", \"rm\": \"Raeto-Romance\", \"ro\":\"Romanian\", \"ru\":\"Russian\",\n",
    "                  \"sr\": \"Serbian\", \"sl\": \"Slovak\", \"sk\": \"Slovenian\", \"es\": \"Spanish\", \"sv\": \"Swedish\", \"tr\": \"Thai\",\n",
    "                  \"th\": \"Turkish\", \"vi\": \"Vietnamese\"}\n",
    "def modify_metadata(metadata):\n",
    "    keys_to_delete = []\n",
    "    for key in metadata.keys():\n",
    "        if metadata[key] is None:\n",
    "            keys_to_delete.append(key)\n",
    "\n",
    "    for key in keys_to_delete:\n",
    "        del metadata[key]\n",
    "    metadata[\"genre_ids\"] = id2name(metadata[\"genre_ids\"])\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0afa2ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = tmdb.Search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82862109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▍                                                                                                                         | 1/50 [00:00<00:07,  6.84it/s]C:\\Users\\pleasework\\PycharmProjects\\MovieAnalysis\\venv\\lib\\site-packages\\torch\\amp\\autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [05:00<00:00,  6.02s/it]\n"
     ]
    }
   ],
   "source": [
    "unembedded_data = {}\n",
    "all_data = []\n",
    "for index, movie in tqdm(enumerate(d_subset), total=len(d_subset)):\n",
    "    #How to make a search using TMDB\n",
    "    tmdb_responses = search.movie(query=movie[0])[\"results\"]\n",
    "    if not len(tmdb_responses):\n",
    "        print(f\"{movie[0]} was not found in db\")\n",
    "        continue\n",
    "    metadata = None\n",
    "    for result in tmdb_responses:\n",
    "        if result[\"release_date\"][:4] == movie[2]:\n",
    "            metadata = result\n",
    "    if metadata == None:\n",
    "        continue\n",
    "    metadata = modify_metadata(metadata)\n",
    "    page_content = get_page_content(movie[1])\n",
    "    chunks = chunk_text(page_content, 384, 20)\n",
    "    chunked_embeddings = dense_model.encode(chunks)\n",
    "    input_ids = tokenizer(\n",
    "        chunks, return_tensors='pt',\n",
    "        padding=True, truncation=True\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        sparse_vecs = sparse_model(\n",
    "            d_kwargs=input_ids.to(device)\n",
    "        )['d_rep'].squeeze()\n",
    "    for index, embedding in enumerate(chunked_embeddings):\n",
    "        new_id = f\"{metadata['id']}-{index}\"\n",
    "        dense_embedding = embedding.tolist()\n",
    "        sparse_vec = sparse_vecs[index]\n",
    "        indices = sparse_vec.nonzero().squeeze().cpu().tolist()  # positions\n",
    "        values = sparse_vec[indices].cpu().tolist()  # weights/scores\n",
    "        # build sparse values dictionary\n",
    "        sparse_values = {\n",
    "            \"indices\": indices,\n",
    "            \"values\": values\n",
    "        }\n",
    "        all_data.append(\n",
    "            {\"id\":new_id,\n",
    "             \"values\":dense_embedding,\n",
    "             \"sparse_values\": sparse_values,\n",
    "             \"metadata\":metadata}\n",
    "        )\n",
    "        unembedded_data[new_id] = chunks[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f6092f",
   "metadata": {},
   "source": [
    "## Use Pinecone to host and search snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48886197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "pinecone.init(api_key = api_keys[\"PINECONE_API_KEY\"], environment=\"gcp-starter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71483cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"movie-finder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b7b8162-9341-4f5e-ae45-77bc4ab4e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create index if it hasn't been created yet"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35aa8977-98c2-40db-8075-2c92b1b684f7",
   "metadata": {},
   "source": [
    "pinecone.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fe05e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=768,\n",
    "        metric=\"dotproduct\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7287cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pinecone.GRPCIndex(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "663fad8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "def batch_list(input_list, n=100):\n",
    "    for i in range(0, len(input_list), n):\n",
    "        yield input_list[i:i + n]\n",
    "counter = 0\n",
    "for batch in batch_list(all_data):\n",
    "    print(counter)\n",
    "    index.upsert(vectors=batch)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494a6d27",
   "metadata": {},
   "source": [
    "## Semantic search with GPT for reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ed73560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = api_keys[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e099f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str):\n",
    "    dense_vec = dense_model.encode(text).tolist()\n",
    "    input_ids = tokenizer(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        sparse_vec = sparse_model(\n",
    "            d_kwargs=input_ids.to(device)\n",
    "        )['d_rep'].squeeze()\n",
    "    indices = sparse_vec.nonzero().squeeze().cpu().tolist()\n",
    "    values = sparse_vec[indices].cpu().tolist()\n",
    "    sparse_dict = {\"indices\": indices, \"values\": values}\n",
    "    return dense_vec, sparse_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81b69860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(ids):\n",
    "    result = []\n",
    "    for id, metadata in ids:\n",
    "        result.append(unembedded_data[id] + metadata)\n",
    "    return \"\\n\".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ccf4b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metadata(metadata):\n",
    "    return f\"==film_title: {metadata['title']}, genre(s): {metadata['genre_ids']}, original language: {language_codes[metadata['original_language']]}, release_date: {metadata['release_date']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "210cf4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(question, k=5):\n",
    "    dense, sparse = encode(question)\n",
    "    xc = index.query(\n",
    "        vector=dense,\n",
    "        sparse_vector=sparse,\n",
    "        top_k=k, \n",
    "        include_metadata=True\n",
    "    )\n",
    "    response_ids_metadata = [(match[\"id\"], format_metadata(match[\"metadata\"])) for match in xc[\"matches\"]]\n",
    "    response = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo-0613\",\n",
    "      messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"Using only the following context:\\n{get_context(response_ids_metadata)}\\nAnswer this question: {question}\"}\n",
    "        ]\n",
    "    )\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8fda23c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'860317-1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmovie with undercover cop and undercover mafia\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[34], line 13\u001b[0m, in \u001b[0;36mget_response\u001b[1;34m(question, k)\u001b[0m\n\u001b[0;32m      3\u001b[0m xc \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mquery(\n\u001b[0;32m      4\u001b[0m     vector\u001b[38;5;241m=\u001b[39mdense,\n\u001b[0;32m      5\u001b[0m     sparse_vector\u001b[38;5;241m=\u001b[39msparse,\n\u001b[0;32m      6\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mk, \n\u001b[0;32m      7\u001b[0m     include_metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m response_ids_metadata \u001b[38;5;241m=\u001b[39m [(match[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m], format_metadata(match[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m])) \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m xc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatches\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m     10\u001b[0m response \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mChatCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     11\u001b[0m   model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-0613\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m   messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m---> 13\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing only the following context:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mget_context(response_ids_metadata)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer this question: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     14\u001b[0m     ]\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[32], line 4\u001b[0m, in \u001b[0;36mget_context\u001b[1;34m(ids)\u001b[0m\n\u001b[0;32m      2\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, metadata \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[1;32m----> 4\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(\u001b[43munembedded_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m metadata)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(result)\n",
      "\u001b[1;31mKeyError\u001b[0m: '860317-1'"
     ]
    }
   ],
   "source": [
    "get_response(\"movie with undercover cop and undercover mafia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2069884",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_response(\"movie where two guys save a gym\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31407e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_response(\"movie where monkey goes to the moon.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50482d0",
   "metadata": {},
   "source": [
    "# Second Functionality - text2SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62761d80",
   "metadata": {},
   "source": [
    "## Scrape Letterbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb82ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal of this aspect of the project is to enable arbitrary queries of the type:\n",
    "    #I want (genre) (language), (release date), (runtime) films that are similar to x,y, ... in (genre),(description),(cast),(crew)\n",
    "# This involves first scraping all data related to cast, genre, themes, etc. films from letterbox and uploading to a db\n",
    "# then use GPT to convert a natural language query into a SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dbd881d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some details of the movies were not included in their page\n",
    "\n",
    "def get_themes(soup):\n",
    "    sections = soup.find_all('section', {'class': 'genre-group'})\n",
    "    section_texts = []\n",
    "    for section in sections:\n",
    "        section_text = section.text.strip()\n",
    "        section_texts.append(section_text)\n",
    "    return section_texts\n",
    "def get_cast(soup):\n",
    "    cast_list_div = soup.find('div', {'class': 'cast-list text-sluglist'})\n",
    "    cast_names = []\n",
    "    if cast_list_div:\n",
    "        actor_tags = cast_list_div.find_all('a', {'class': 'text-slug tooltip'})\n",
    "        for tag in actor_tags:\n",
    "            cast_names.append(tag.text.lower())\n",
    "    return cast_names\n",
    "def get_genres(soup):\n",
    "    genres_div = soup.find('div', {'class': 'text-sluglist capitalize'})\n",
    "    genre_names = []\n",
    "    if genres_div:\n",
    "        genre_tags = genres_div.find_all('a', {'class': 'text-slug'})\n",
    "        for tag in genre_tags:\n",
    "            genre_names.append(tag.text)\n",
    "    return genre_names\n",
    "def get_studios(soup):\n",
    "    h3_studios = soup.find('h3', text=lambda t: 'Studios' in t if t else False)\n",
    "    studio_names = []\n",
    "    if h3_studios:\n",
    "        studios_div = h3_studios.find_next('div', {'class': 'text-sluglist'})\n",
    "        if studios_div:\n",
    "            studio_tags = studios_div.find_all('a', {'class': 'text-slug'})\n",
    "            for tag in studio_tags:\n",
    "                studio_names.append(tag.text)\n",
    "    return studio_names\n",
    "\n",
    "def get_original_language(soup):\n",
    "    h3_language = soup.find('h3', text=lambda t: 'Original Language' in t if t else False)\n",
    "    original_languages = []\n",
    "    if h3_language:\n",
    "        language_div = h3_language.find_next('div', {'class': 'text-sluglist'})\n",
    "\n",
    "        if language_div:\n",
    "            language_tags = language_div.find_all('a', {'class': 'text-slug'})\n",
    "            for tag in language_tags:\n",
    "                original_languages.append(tag.text)\n",
    "    return original_languages[0].lower() if len(original_languages) else \"\"\n",
    "\n",
    "def get_similar(soup):\n",
    "    target_links = []\n",
    "    div_tags = soup.find_all('div', {'class': 'really-lazy-load'})\n",
    "    for div in div_tags:\n",
    "        link = div.get('data-target-link')\n",
    "        if link:\n",
    "            target_links.append(link.replace(\"/film/\", \"\").strip(\"/\"))\n",
    "    return target_links\n",
    "\n",
    "def get_release_year(soup):\n",
    "    try:\n",
    "        year = soup.select_one('section#featured-film-header small.number a').text\n",
    "    except AttributeError:\n",
    "        print('Release year was not available on the page')\n",
    "        year = ''\n",
    "        pass\n",
    "    return year\n",
    "\n",
    "def get_runtime(soup):\n",
    "    p_tag = soup.find('p', {'class': ['text-link', 'text-footer']})\n",
    "    if p_tag:\n",
    "        text_content = p_tag.text\n",
    "        match = re.search(r'(\\d+)\\s*mins', text_content)\n",
    "        if match:\n",
    "            mins = int(match.group(1))   \n",
    "            return mins\n",
    "        \n",
    "def get_watched_stats(soup):\n",
    "    try:\n",
    "        watched = soup.find('li', {'class': 'js-route-watches'}).find('a', {'class': 'tooltip'})['title'].split()[0].replace(',', '')\n",
    "    except:\n",
    "        watched = 0\n",
    "    try:\n",
    "        fans = soup.find('li', {'class': 'js-route-fans'}).find('a', {'class': 'tooltip'})['title'].split()[0].replace(',', '')\n",
    "    except:\n",
    "        fans = 0\n",
    "    try:\n",
    "        likes = soup.find('li', {'class': 'js-route-likes'}).find('a', {'class': 'tooltip'})['title'].split()[0].replace(',', '')\n",
    "    except:\n",
    "        likes = 0\n",
    "    try:\n",
    "        reviews = soup.find('li', {'class': 'js-route-reviews'}).find('a', {'class': 'tooltip'})['title'].split()[0].replace(',', '')\n",
    "    except:\n",
    "        reviews = 0\n",
    "    return watched, fans, likes, reviews\n",
    "\n",
    "def get_crew(soup):\n",
    "    roles_and_names = {}\n",
    "    for h3_tag in soup.find_all('h3'):\n",
    "        role_span = h3_tag.find('span', {'class': 'crewrole -full'})\n",
    "        if role_span:\n",
    "            role_full = role_span.text\n",
    "            name_div = h3_tag.find_next('div', {'class': 'text-sluglist'})\n",
    "            if name_div:\n",
    "                name_tags = name_div.find_all('a', {'class': 'text-slug'})\n",
    "                names = [name_tag.text.lower() for name_tag in name_tags]\n",
    "                roles_and_names[role_full.lower()] = names\n",
    "    return roles_and_names\n",
    "\n",
    "def modify_movie(movie_title):\n",
    "    return ''.join(char for char in movie_title if char not in string.punctuation).lower().replace(' ', '-')\n",
    "\n",
    "def find_movie(movie, url_base):\n",
    "    movie_title = movie[0]\n",
    "    year = movie[2]\n",
    "    modified_title = modify_movie(movie_title)\n",
    "    with_year = f\"{modified_title}-{year}\"\n",
    "    soup = get_soup(f\"{url_base}{with_year}\")\n",
    "    if \"Not Found\" in soup.title.string:\n",
    "        soup = get_soup(f\"{url_base}{modified_title}\")\n",
    "        if \"Not Found\" in soup.title.string:\n",
    "            return None, None, None\n",
    "        else:\n",
    "            validate_year = get_release_year(soup)\n",
    "            if validate_year == year:\n",
    "                return soup, modified_title, year\n",
    "            else:\n",
    "                return None, None, None\n",
    "    else:\n",
    "        return soup,with_year,year\n",
    "\n",
    "def scrape_all(movie, url_base):\n",
    "    soup, letterbox_title, year = find_movie(movie, url_base)\n",
    "    if soup is None:\n",
    "        return None\n",
    "    scraped_dict = {}\n",
    "    scraped_dict[\"title\"] = movie[0]\n",
    "    scraped_dict[\"genres\"] = get_genres(soup)\n",
    "    scraped_dict[\"studios\"] = get_studios(soup)\n",
    "    scraped_dict[\"original_language\"] = get_original_language(soup)\n",
    "    scraped_dict[\"release year\"] = year\n",
    "    scraped_dict[\"runtime\"] = get_runtime(soup)\n",
    "    crew_dict = get_crew(soup)\n",
    "    crew_dict[\"actor\"] = get_cast(soup)\n",
    "    scraped_dict[\"crew\"] = crew_dict\n",
    "    scraped_dict[\"themes\"] = get_themes(get_soup(f\"{url_base}{letterbox_title}/themes/\"))\n",
    "    all_similar = get_similar(get_soup(f\"{url_base}{letterbox_title}/similar\"))\n",
    "    scraped_dict[\"similar\"] = [film for film in all_similar if film != letterbox_title]\n",
    "    stats = get_watched_stats(get_soup(f\"{url_base}{letterbox_title}/members\"))\n",
    "    scraped_dict[\"num_watched\"] = stats[0]\n",
    "    scraped_dict[\"num_fans\"] = stats[1]\n",
    "    scraped_dict[\"num_liked\"] = stats[2]\n",
    "    scraped_dict[\"num_reviewed\"] = stats[3]\n",
    "    return {letterbox_title: scraped_dict}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "00eeed9a-e6b9-4e4a-b3ac-31f7611575e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0cc41d42-a063-43a0-a0cd-8bcf221fb004",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Have this loop to make randomized user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "02bf8c3d-9c2e-485d-8953-422964b55128",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Troubleshooting scraping\n",
    "scrapedList=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ea33f028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                  | 0/5000 [00:00<?, ?it/s]C:\\Users\\pleasework\\AppData\\Local\\Temp\\ipykernel_14092\\4138716162.py:27: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  h3_studios = soup.find('h3', text=lambda t: 'Studios' in t if t else False)\n",
      "C:\\Users\\pleasework\\AppData\\Local\\Temp\\ipykernel_14092\\4138716162.py:38: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  h3_language = soup.find('h3', text=lambda t: 'Original Language' in t if t else False)\n",
      "  0%|▏                                                                                                                       | 8/5000 [00:13<2:07:08,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release year was not available on the page\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|████▊                                                                                                                 | 204/5000 [07:07<2:10:02,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release year was not available on the page\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████████▎                                                                                                         | 523/5000 [18:18<2:13:11,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release year was not available on the page\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██████████████████████████▎                                                                                          | 1126/5000 [40:37<3:04:34,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release year was not available on the page\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████████████████                                                                                  | 1496/5000 [54:18<1:37:43,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release year was not available on the page\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|███████████████████████████████████████████████▌                                                                   | 2070/5000 [1:16:06<1:46:37,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release year was not available on the page\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████████████████████████████▎                                                                | 2186/5000 [1:20:35<1:23:41,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release year was not available on the page\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████████████████████████████████████████████████████████████████                                                  | 2830/5000 [1:45:46<1:18:33,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release year was not available on the page\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|█████████████████████████████████████████████████████████████████████████▍                                           | 3140/5000 [1:57:03<43:16,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release year was not available on the page\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████████████████▍                                   | 3481/5000 [2:10:11<52:46,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release year was not available on the page\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|█████████████████████████████████████████████████████████████████████████████████████                                | 3633/5000 [2:15:26<34:19,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release year was not available on the page\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|████████████████████████████████████████████████████████████████████████████████████████▍                            | 3782/5000 [2:21:06<31:03,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release year was not available on the page\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████████████████████████████████████████████████████████████████████████████████████████▏                         | 3896/5000 [2:25:00<25:38,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release year was not available on the page\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|████████████████████████████████████████████████████████████████████████████████████████████▊                        | 3969/5000 [2:27:33<30:34,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release year was not available on the page\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|███████████████████████████████████████████████████████████████████████████████████████████████▏                     | 4066/5000 [2:31:30<31:23,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release year was not available on the page\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|██████████████████████████████████████████████████████████████████████████████████████████████████▏                  | 4195/5000 [2:36:01<28:30,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release year was not available on the page\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 4853/5000 [3:00:39<02:43,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release year was not available on the page\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [3:06:06<00:00,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3:06:06.642549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "url_base = \"https://letterboxd.com/film/\"\n",
    "for movie in tqdm(all_movies[:5000]):\n",
    "    scrapedList.append(movie)\n",
    "    if movie not in full_dict:\n",
    "        full_dict[movie] = scrape_all(movie, url_base)\n",
    "    else:\n",
    "        pass\n",
    "print(datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3ba58f1c-4fcd-4396-a6ad-aa1a640932d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## credit to user Tms91 from the following thread: https://stackoverflow.com/questions/7001606/json-serialize-a-dictionary-with-tuples-as-key\n",
    "\n",
    "def json_dumps_dict_having_tuple_as_key(dict_having_tuple_as_key):\n",
    "    if not isinstance(dict_having_tuple_as_key, dict):\n",
    "        raise Exception('Error using json_dumps_dict_having_tuple_as_key: The input variable is not a dictionary.')  \n",
    "    list_of_dicts_having_key_and_value_as_keys = [{'key': k, 'value': v} for k, v in dict_having_tuple_as_key.items()]\n",
    "    json_array_having_key_and_value_as_keys = json.dumps(list_of_dicts_having_key_and_value_as_keys)\n",
    "    return json_array_having_key_and_value_as_keys\n",
    "\n",
    "def json_loads_dictionary_split_into_key_and_value_as_keys_and_underwent_json_dumps(json_array_having_key_and_value_as_keys):\n",
    "    list_of_dicts_having_key_and_value_as_keys = json.loads(json_array_having_key_and_value_as_keys)\n",
    "    if not all(['key' in diz for diz in list_of_dicts_having_key_and_value_as_keys]) and all(['value' in diz for diz in list_of_dicts_having_key_and_value_as_keys]):\n",
    "        raise Exception('Error using json_loads_dictionary_split_into_key_and_value_as_keys_and_underwent_json_dumps: at least one dictionary in list_of_dicts_having_key_and_value_as_keys ismissing key \"key\" or key \"value\".')\n",
    "    dict_having_tuple_as_key = {}\n",
    "    for dict_having_key_and_value_as_keys in list_of_dicts_having_key_and_value_as_keys:\n",
    "        dict_having_tuple_as_key[ tuple(dict_having_key_and_value_as_keys['key']) ] = dict_having_key_and_value_as_keys['value']\n",
    "    return dict_having_tuple_as_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ccf468d0-886f-436b-bc81-ec0b2be9d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = json_dumps_dict_having_tuple_as_key(full_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c3a59dc5-bd96-4b93-9033-d8716fd70fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fulldict.json\", \"w\") as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "facadf44-b096-4443-86ed-8f812af65c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('fulldict.json', 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    json_object = json.load(openfile)\n",
    " \n",
    "print(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97597028-8157-437e-8281-291fa5eb75de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generateUsers(numUsers, listOfUsers, fullMovieDict):\n",
    "    userDict = {}\n",
    "    for i in range(i, numUsers):\n",
    "        listOfMovies = []\n",
    "        for j in range(j,10):\n",
    "            listOfMovies.append(random.choice(list(fullMovieDict.items())))\n",
    "        userDict[listOfUsers[i]] = listOfMovies\n",
    "    return userDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762837af-3498-4cdb-b14b-b317902b13e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Given your top 10 movies, return the most frequently appearing \"similar movies\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9fec46-c010-44b0-8ece-f92760dc1577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import operator\n",
    "def getitem(d,key):\n",
    "    return reduce(operator.getitem, key, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b34629-aa6b-45b0-8ece-0ed9f984b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = []\n",
    "counter = 0\n",
    "for key in full_dict:\n",
    "    try:\n",
    "        # Since the nested key name is somewhat variable and the positions are always the same, jury rigged a simple solution \n",
    "        check.append(getitem(full_dict[key][list(full_dict[key].keys())[0]], ['crew', 'director'])[0])\n",
    "        counter += 1\n",
    "    except AttributeError:\n",
    "        print('There was a problem with the way the data was scraped')\n",
    "        check.append('user'+str(counter))\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c68b36d-ae63-4c96-9e62-2aae01e0b0ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_errors = []\n",
    "similar_movies_dict = {}\n",
    "for key in full_dict:\n",
    "    try:\n",
    "        similar_movies_dict[key] = full_dict[key][list(full_dict[key].keys())[0]]['similar']\n",
    "    except KeyError:\n",
    "        print('Letterboxd probably did not include a list of similar movies for this title')\n",
    "        pass\n",
    "    except TypeError:\n",
    "        # Some urls don't link to an actual title, removing those from the dict\n",
    "        print('There was an error with the url, there is no individual title by that name')\n",
    "        dict_errors.append(key)\n",
    "        pass\n",
    "for key in dict_errors:\n",
    "    del full_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af35861c-cede-4051-bb6f-730d4b0b85ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function literally counts how many times each element shows up in a list. \n",
    "# So if we just add the lists of similar movies from the top 10 into one, that should work\n",
    "from collections import Counter\n",
    "Counter(sum(similar_movies_dict.values(), [])).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1128d986",
   "metadata": {},
   "source": [
    "## Upload data to PostgresSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2632ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfbc1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\", \n",
    "    user=api_keys[\"POSTGRES_USER\"],\n",
    "    password=api_keys[\"POSTGRES_PASSWORD\"],\n",
    "    port=\"5432\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS movies (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    title VARCHAR(255),\n",
    "    letterbox_title VARCHAR(255) UNIQUE,\n",
    "    original_language VARCHAR(50),\n",
    "    release_year INT,\n",
    "    runtime INT,\n",
    "    num_watched INT,\n",
    "    num_fans INT,\n",
    "    num_liked INT,\n",
    "    num_reviewed INT\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS genres (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    genre_name VARCHAR(255) UNIQUE\n",
    ");\n",
    "\"\"\")\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS roles (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    role_name VARCHAR(255) UNIQUE\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS themes (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    theme_name VARCHAR(255) UNIQUE\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS studios (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    studio_name VARCHAR(255) UNIQUE\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS similar_movies (\n",
    "    movie_id INT,\n",
    "    similar_movie_id INT,\n",
    "    PRIMARY KEY (movie_id, similar_movie_id),\n",
    "    FOREIGN KEY (movie_id) REFERENCES movies (id),\n",
    "    FOREIGN KEY (similar_movie_id) REFERENCES movies (id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS movie_crew (\n",
    "    movie_id INT,\n",
    "    role_id INT,\n",
    "    crew_name VARCHAR(255),\n",
    "    PRIMARY KEY (movie_id, role_id, crew_name),\n",
    "    FOREIGN KEY (movie_id) REFERENCES movies (id),\n",
    "    FOREIGN KEY (role_id) REFERENCES roles (id)\n",
    ");\n",
    "\"\"\")\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS movie_studios (\n",
    "    movie_id INT,\n",
    "    studio_id INT,\n",
    "    PRIMARY KEY (movie_id, studio_id),\n",
    "    FOREIGN KEY (movie_id) REFERENCES movies (id),\n",
    "    FOREIGN KEY (studio_id) REFERENCES studios (id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS movie_genres (\n",
    "    movie_id INT,\n",
    "    genre_id INT,\n",
    "    PRIMARY KEY (movie_id, genre_id),\n",
    "    FOREIGN KEY (movie_id) REFERENCES movies (id),\n",
    "    FOREIGN KEY (genre_id) REFERENCES genres (id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS movie_themes (\n",
    "    movie_id INT,\n",
    "    theme_id INT,\n",
    "    PRIMARY KEY (movie_id, theme_id),\n",
    "    FOREIGN KEY (movie_id) REFERENCES movies (id),\n",
    "    FOREIGN KEY (theme_id) REFERENCES themes (id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "dataset = full_dict\n",
    "\n",
    "for key, movie_data in dataset.items():\n",
    "    cur.execute(\"\"\"\n",
    "    INSERT INTO movies (title, letterbox_title, original_language, release_year, runtime, num_watched, num_fans, num_liked, num_reviewed)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING id;\n",
    "    \"\"\", (\n",
    "        movie_data.get('title', None),\n",
    "        key,\n",
    "        movie_data.get('original_language', None),\n",
    "        movie_data.get('release year', None),\n",
    "        movie_data.get('runtime', None),\n",
    "        movie_data.get('num_watched', None),\n",
    "        movie_data.get('num_fans', None),\n",
    "        movie_data.get('num_liked', None),\n",
    "        movie_data.get('num_reviewed', None)\n",
    "    ))\n",
    "\n",
    "    movie_id = cur.fetchone()[0]\n",
    "\n",
    "    for genre in movie_data.get('genres', []):\n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO genres (genre_name) VALUES (%s) ON CONFLICT (genre_name) DO NOTHING;\n",
    "        \"\"\", (genre,))\n",
    "\n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO movie_genres (movie_id, genre_id) SELECT %s, id FROM genres WHERE genre_name = %s;\n",
    "        \"\"\", (movie_id, genre))\n",
    "        \n",
    "    for studio in movie_data.get('studios', []):\n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO studios (studio_name) VALUES (%s) ON CONFLICT (studio_name) DO NOTHING;\n",
    "        \"\"\", (studio,))\n",
    "\n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO movie_studios (movie_id, studio_id) SELECT %s, id FROM studios WHERE studio_name = %s;\n",
    "        \"\"\", (movie_id, studio))\n",
    "        \n",
    "    for theme in movie_data.get('themes', []):\n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO themes (theme_name) VALUES (%s) ON CONFLICT (theme_name) DO NOTHING;\n",
    "        \"\"\", (theme,))\n",
    "\n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO movie_themes (movie_id, theme_id) SELECT %s, id FROM themes WHERE theme_name = %s;\n",
    "        \"\"\", (movie_id, theme))\n",
    "        \n",
    "    for role, crew_names in movie_data.get('crew', {}).items():\n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO roles (role_name) VALUES (%s) ON CONFLICT (role_name) DO NOTHING;\n",
    "        \"\"\", (role,))\n",
    "\n",
    "        for crew_name in crew_names:\n",
    "            cur.execute(\"\"\"\n",
    "            INSERT INTO movie_crew (movie_id, role_id, crew_name)\n",
    "            SELECT %s, id, %s FROM roles WHERE role_name = %s;\n",
    "            \"\"\", (movie_id, crew_name, role))\n",
    "    \n",
    "    for similar_movie in movie_data.get('similar', []):\n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO similar_movies (movie_id, similar_movie_id)\n",
    "        SELECT %s, id FROM movies WHERE letterbox_title = %s;\n",
    "        \"\"\", (movie_id, similar_movie))\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065f5447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63ee0e7-025f-47b6-87d8-b20bf1152cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
