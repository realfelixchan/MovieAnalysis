{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "96d3c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "3154a13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['TMDB_API_KEY', 'PINECONE_API_KEY', 'OPENAI_API_KEY', 'POSTGRES_USER', 'POSTGRES_PASSWORD'])\n"
     ]
    }
   ],
   "source": [
    "#Save a json file with api keys for tmdb, openai, pinecone, and postgres username/password\n",
    "api_keys = json.load(open(\"api_keys.json\"))\n",
    "print(api_keys.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dfcb49",
   "metadata": {},
   "source": [
    "## Misc functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "92d61391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "def closest_key(query, dictionary):\n",
    "    closest_key = min(dictionary.keys(), key=lambda k: Levenshtein.distance(query, k))\n",
    "    return dictionary[closest_key]\n",
    "def find_repeated_first_elements(tuples_list):\n",
    "    element_dict = defaultdict(list)\n",
    "    repeated_elements = {}\n",
    "    for t in tuples_list:\n",
    "        element_dict[t[0]].append(t)\n",
    "    \n",
    "    for key, value in element_dict.items():\n",
    "        if len(value) > 1:\n",
    "            repeated_elements[key] = value\n",
    "    \n",
    "    return repeated_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252d60ea",
   "metadata": {},
   "source": [
    "# First Functionality - semantic search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc3730",
   "metadata": {},
   "source": [
    "## Scrape from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a47e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We get a list of all movies from wikipedia which serves as a source of information that will be utilized in the semantic\n",
    "#search aspect of this tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad84a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_urls = [\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_A\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_B\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_C\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_D\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_E\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_F\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_G\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_H\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_I\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_J%E2%80%93K\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_L\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_M\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_N%E2%80%93O\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_P\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_Q%E2%80%93R\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_S\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_T\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_U%E2%80%93W\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_X%E2%80%93Z\"\n",
    "]\n",
    "def get_soup(url):\n",
    "    response = requests.get(url)\n",
    "    page_content = response.content\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    return soup\n",
    "def get_all_movies(url):\n",
    "    soup = get_soup(url)\n",
    "    movie_info = []\n",
    "\n",
    "    for li in soup.find_all('li'):\n",
    "        a = li.find('a')\n",
    "        if a:\n",
    "            title = a.get('title')\n",
    "            href = a.get('href')\n",
    "\n",
    "            year_match = re.search(r'\\((\\d{4})\\)', li.text)\n",
    "            year = year_match.group(1) if year_match else 'Unknown'\n",
    "            if year == \"Unknown\":\n",
    "                continue\n",
    "            try:\n",
    "                title = re.sub(r' \\([^)]*(film|movie)[^)]*\\)', '', title)\n",
    "            except:\n",
    "                continue\n",
    "            movie_info.append((title, href, year))\n",
    "    \n",
    "    for li in soup.find_all('li'):\n",
    "        main_title = li.find('i').text if li.find('i') else ''\n",
    "\n",
    "        a_tags = li.find_all('a')\n",
    "        for a in a_tags:\n",
    "            title = a.get('title', main_title)\n",
    "            href = a.get('href', '')\n",
    "\n",
    "            year_match = re.search(r'(\\d{4})', a.text)\n",
    "            year = year_match.group(1) if year_match else 'Unknown'\n",
    "            if year == \"Unknown\":\n",
    "                continue\n",
    "            title = re.sub(r' \\([^)]*(film|movie)[^)]*\\)', '', title)\n",
    "            movie_info.append((title, href, year))\n",
    "    return list(set(movie_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ddbd96c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_movies = []\n",
    "for url in all_urls:\n",
    "    all_movies = all_movies + get_all_movies(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "fdff55c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(all_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "22fd2837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Zapata's Gang\", '/wiki/Zapata%27s_Gang', '1914'),\n",
       " ('Shakedown', '/wiki/Shakedown_(1950_film)', '1950'),\n",
       " ('Lost and Delirious', '/wiki/Lost_and_Delirious', '2001'),\n",
       " ('Rabindranath Tagore', '/wiki/Rabindranath_Tagore_(film)', '1961'),\n",
       " ('Shrooms', '/wiki/Shrooms_(film)', '2007')]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_movies[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "686ba120",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_movies = get_all_movies(\"https://en.wikipedia.org/wiki/List_of_films:_D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbaa3ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_subset = d_movies[800:850]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9135d4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The Departed', '/wiki/The_Departed', '2006'),\n",
       " ('Samurai II: Duel at Ichijoji Temple',\n",
       "  '/wiki/Samurai_II:_Duel_at_Ichijoji_Temple',\n",
       "  '1955'),\n",
       " ('Delirious', '/wiki/Delirious_(2006_film)', '2006'),\n",
       " ('Devdas', '/wiki/Devdas_(1937_film)', '1937'),\n",
       " ('Dumbbells', '/wiki/Dumbbells_(film)', '2014'),\n",
       " ('Death Wish', '/wiki/Death_Wish_(2018_film)', '2018'),\n",
       " ('The Deluge', '/wiki/The_Deluge_(film)', '1974'),\n",
       " ('Dvořák - In Love?', '/wiki/Dvo%C5%99%C3%A1k_-_In_Love%3F', '1988'),\n",
       " ('The Doll', '/wiki/The_Doll_(2008_film)', '2008'),\n",
       " ('Double Tap', '/wiki/Double_Tap_(film)', '2000'),\n",
       " ('Dragon Ball: Mystical Adventure',\n",
       "  '/wiki/Dragon_Ball:_Mystical_Adventure',\n",
       "  '1988'),\n",
       " ('Daens', '/wiki/Daens_(film)', '1992'),\n",
       " ('The Demon', '/wiki/The_Demon_(1979_film)', '1981'),\n",
       " ('The Discreet Charm of the Bourgeoisie',\n",
       "  '/wiki/The_Discreet_Charm_of_the_Bourgeoisie',\n",
       "  '1972'),\n",
       " ('Double Dragon', '/wiki/Double_Dragon_(film)', '1994'),\n",
       " ('Dandelion', '/wiki/Dandelion_(2004_film)', '2004'),\n",
       " ('Dwandha Yudham', '/wiki/Dwandha_Yudham', '1981'),\n",
       " ('Devi', '/wiki/Devi_(1999_film)', '1999'),\n",
       " ('Do Chattane', '/wiki/Do_Chattane', '1974'),\n",
       " ('Devotion', '/wiki/Devotion_(1929_film)', '1929'),\n",
       " ('The Deal', '/wiki/The_Deal_(2005_film)', '2005'),\n",
       " ('Dennis the Menace', '/wiki/Dennis_the_Menace_(1993_film)', '1993'),\n",
       " ('Disappearance at Clifton Hill',\n",
       "  '/wiki/Disappearance_at_Clifton_Hill',\n",
       "  '2019'),\n",
       " ('Dolls', '/wiki/Dolls_(2002_film)', '2002'),\n",
       " ('Déjà Vu', '/wiki/D%C3%A9j%C3%A0_Vu_(2006_film)', '2006'),\n",
       " ('Dweller', '/wiki/Dweller_(film)', '2002'),\n",
       " ('Dumm Dumm Dumm', '/wiki/Dumm_Dumm_Dumm', '2001'),\n",
       " ('Dangerous Lies', '/wiki/Dangerous_Lies_(2020_film)', '2020'),\n",
       " ('Delivered', '/wiki/Delivered', '1999'),\n",
       " ('Dennis the Menace Strikes Again',\n",
       "  '/wiki/Dennis_the_Menace_Strikes_Again',\n",
       "  '1998'),\n",
       " ('Day for Night', '/wiki/Day_for_Night_(film)', '1973'),\n",
       " ('Dog', '/wiki/Dog_(2022_film)', '2022'),\n",
       " ('Double Exposure', '/wiki/Double_Exposure_(2014_film)', '2014'),\n",
       " (\"Don't Tell Her It's Me\", '/wiki/Don%27t_Tell_Her_It%27s_Me', '1990'),\n",
       " ('The Diary of a Chambermaid',\n",
       "  '/wiki/The_Diary_of_a_Chambermaid_(1946_film)',\n",
       "  '1946'),\n",
       " ('Double Crossbones', '/wiki/Double_Crossbones', '1951'),\n",
       " ('Daisy Kenyon', '/wiki/Daisy_Kenyon', '1947'),\n",
       " ('Doubtful', '/wiki/Doubtful_(film)', '2017'),\n",
       " ('Devotion', '/wiki/Devotion_(1931_film)', '1931'),\n",
       " ('Deal of the Century', '/wiki/Deal_of_the_Century', '1983'),\n",
       " ('Dünyayı Kurtaran Adam', '/wiki/D%C3%BCnyay%C4%B1_Kurtaran_Adam', '1982'),\n",
       " ('Dynamite Ranch', '/wiki/Dynamite_Ranch', '1932'),\n",
       " ('Dark Is the Night', '/wiki/Dark_Is_the_Night_(1945_film)', '1945'),\n",
       " ('Lego DC Super Hero Girls: Brain Drain',\n",
       "  '/wiki/Lego_DC_Super_Hero_Girls:_Brain_Drain',\n",
       "  '2017'),\n",
       " ('Dishonored Lady', '/wiki/Dishonored_Lady', '1947'),\n",
       " ('Detroit Rock City', '/wiki/Detroit_Rock_City_(film)', '1999'),\n",
       " ('Dhobi Ghat', '/wiki/Dhobi_Ghat_(film)', '2010'),\n",
       " ('Dragon Ball: Curse of the Blood Rubies',\n",
       "  '/wiki/Dragon_Ball:_Curse_of_the_Blood_Rubies',\n",
       "  '1986'),\n",
       " ('Dragon Ball Z: Bio-Broly', '/wiki/Dragon_Ball_Z:_Bio-Broly', '1994'),\n",
       " ('Dynamite Brothers', '/wiki/Dynamite_Brothers', '1974')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ec15eb",
   "metadata": {},
   "source": [
    "## Obtaining snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "6c8f8047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We scrape the particular wikipedia pages and scrape a \"snippet\" chunk of text to reference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "215fcede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    if overlap >= n:\n",
    "        return \"Overlap must be smaller than chunk size\"\n",
    "    length = len(text)\n",
    "    start = 0\n",
    "    chunks = []\n",
    "    while start < length:\n",
    "        end = min(start + n, length)\n",
    "        chunks.append(text[start:end])\n",
    "        start += n - overlap\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "def get_page_content(href):\n",
    "    soup = get_soup(f'https://en.wikipedia.org/{href}')\n",
    "    article_container = soup.find('div', {'id': 'mw-content-text'})\n",
    "    article_text = ''\n",
    "    for paragraph in article_container.find_all('p'):\n",
    "        article_text += paragraph.text\n",
    "    infobox = soup.find('table', {'class': 'infobox'})\n",
    "\n",
    "    infobox_text = \"\"\n",
    "\n",
    "    if infobox:\n",
    "        for row in infobox.find_all('tr'):\n",
    "            # Extract the header if available\n",
    "            th = row.find('th')\n",
    "            if th:\n",
    "                infobox_text += th.get_text() + \"\\n\"\n",
    "\n",
    "            # Extract the data in the row\n",
    "            td = row.find('td')\n",
    "            if td:\n",
    "                infobox_text += td.get_text() + \"\\n\"\n",
    "    article_text += infobox_text\n",
    "    div_tag = soup.find('div', {'class': 'div-col'})\n",
    "    ul_tag = div_tag.find('ul') if div_tag else None\n",
    "    list_text = \"\"\n",
    "    if ul_tag:\n",
    "        for li_tag in ul_tag.find_all('li'):\n",
    "            list_text += li_tag.get_text()\n",
    "    article_text += list_text\n",
    "    return article_text\n",
    "\n",
    "def get_chunked_embeddings(href, embedding_model):\n",
    "    page_content = get_page_content(href)\n",
    "    embeddings = embedding_model.encode(chunker(page_content))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4cee8a",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "574a3f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding models allow us to encode our sentences into vectors, particularly we will be using a combination of\n",
    "#sparse and dense embedding models for fine grained semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "a92e9ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util\n",
    "from splade.models.transformer_rep import Splade\n",
    "import torch\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f4c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "478b4e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8859]])\n",
      "tensor([[0.8596]])\n"
     ]
    }
   ],
   "source": [
    "#intutive embedding test\n",
    "test_model = SentenceTransformer('all-mpnet-base-v2',device=device)\n",
    "query_embedding = test_model.encode(\"thriller, crime\")\n",
    "passage_embedding = test_model.encode(\"thriller, drama\")\n",
    "passage_embedding2 = test_model.encode(\"thriller, comedy\")\n",
    "print(util.dot_score(query_embedding, passage_embedding))\n",
    "print(util.dot_score(query_embedding, passage_embedding2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a42272b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_model = SentenceTransformer(\n",
    "    'msmarco-bert-base-dot-v5',\n",
    "    device=device\n",
    ")\n",
    "dense_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12660d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_model_id = 'naver/splade-cocondenser-ensembledistil'\n",
    "\n",
    "sparse_model = Splade(sparse_model_id, agg='max')\n",
    "sparse_model.to(device)\n",
    "sparse_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5015d62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(sparse_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7fe0be",
   "metadata": {},
   "source": [
    "## Scrape TMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "f5284718",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This was the first site that I found that gave metadata information for movies, letterbox may be more comprehensive\n",
    "#but this information is suitable for this first functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "cb650654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tmdbsimple as tmdb\n",
    "tmdb.API_KEY = api_keys[\"TMDB_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "0d8bd116",
   "metadata": {},
   "outputs": [],
   "source": [
    "#genre codes tmdb uses\n",
    "movie_genre_dict = {28:\"Action\",12:\"Adventure\",16:\"Animation\", 35:\"Comedy\", 80:\"Crime\", 99:\"Documentary\",18:\"Drama\", 10751:\"Family\",14:\"Fantasy\",36:\"History\",27:\"Horror\",10402:\"Music\",9648:\"Mystery\",10749:\"Romance\",878:\"Science Fiction\",10770:\"TV Movie\", 53:\"Thriller\", 10752:\"War\",37:\"Western\"}\n",
    "def id2name(id_list):\n",
    "    return [movie_genre_dict[idz] for idz in id_list]\n",
    "#language codes tmdb uses\n",
    "language_codes = {\"af\":\"Afrikaans\", \"sq\":\"Albanian\", \"ar\":\"Arabic\", \"eu\": \"Basque\", \"bg\":\"Bulgarian\",\n",
    "                  \"ca\":\"Catalan\", \"cn\":\"Chinese\", \"zh\":\"Chinese\", \"hr\":\"Croatian\", \"cs\":\"Czech\", \"da\": \"Danish\", \"nl\":\"Dutch\",\n",
    "                  \"en\":\"English\", \"et\": \"Estonian\", \"fi\":\"Finnish\", \"fr\":\"French\", \"de\": \"German\", \"el\": \"Greek\",\n",
    "                  \"he\":\"Hebrew\", \"hi\": \"Hindi\", \"hu\":\"Hungarian\", \"is\":\"Icelandic\", \"in\": \"Indonesian\", \"it\": \"Italian\",\n",
    "                  \"ja\": \"Japanese\", \"ko\": \"Korean\", \"lv\": \"Latvian\", \"lt\":\"Lithuanian\", \"mk\": \"Macedonian\", \"ms\": \"malay\",\n",
    "                  \"no\": \"Norwegian\", \"pl\":\"Polish\", \"pt\": \"Portugese\", \"rm\": \"Raeto-Romance\", \"ro\":\"Romanian\", \"ru\":\"Russian\",\n",
    "                  \"sr\": \"Serbian\", \"sl\": \"Slovak\", \"sk\": \"Slovenian\", \"es\": \"Spanish\", \"sv\": \"Swedish\", \"tr\": \"Thai\",\n",
    "                  \"th\": \"Turkish\", \"vi\": \"Vietnamese\"}\n",
    "def modify_metadata(metadata):\n",
    "    keys_to_delete = []\n",
    "    for key in metadata.keys():\n",
    "        if metadata[key] is None:\n",
    "            keys_to_delete.append(key)\n",
    "\n",
    "    for key in keys_to_delete:\n",
    "        del metadata[key]\n",
    "    metadata[\"genre_ids\"] = id2name(metadata[\"genre_ids\"])\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afa2ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = tmdb.Search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "82862109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████████████████████████████████████▎                                                                                                                                                                              | 9/50 [00:14<00:35,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dvořák - In Love? was not found in db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|████████████████████████████████████████████████████████████████████████                                                                                                                                            | 17/50 [00:25<00:39,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dwandha Yudham was not found in db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                               | 20/50 [00:27<00:21,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do Chattane was not found in db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [01:13<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "unembedded_data = {}\n",
    "all_data = []\n",
    "for index, movie in tqdm(enumerate(d_subset), total=len(d_subset)):\n",
    "    #How to make a search using TMDB\n",
    "    tmdb_responses = search.movie(query=movie[0])[\"results\"]\n",
    "    if not len(tmdb_responses):\n",
    "        print(f\"{movie[0]} was not found in db\")\n",
    "        continue\n",
    "    metadata = None\n",
    "    for result in tmdb_responses:\n",
    "        if result[\"release_date\"][:4] == movie[2]:\n",
    "            metadata = result\n",
    "    if metadata == None:\n",
    "        continue\n",
    "    metadata = modify_metadata(metadata)\n",
    "    page_content = get_page_content(movie[1])\n",
    "    chunks = chunk_text(page_content, 384, 20)\n",
    "    chunked_embeddings = dense_model.encode(chunks)\n",
    "    input_ids = tokenizer(\n",
    "        chunks, return_tensors='pt',\n",
    "        padding=True, truncation=True\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        sparse_vecs = sparse_model(\n",
    "            d_kwargs=input_ids.to(device)\n",
    "        )['d_rep'].squeeze()\n",
    "    for index, embedding in enumerate(chunked_embeddings):\n",
    "        new_id = f\"{metadata['id']}-{index}\"\n",
    "        dense_embedding = embedding.tolist()\n",
    "        sparse_vec = sparse_vecs[index]\n",
    "        indices = sparse_vec.nonzero().squeeze().cpu().tolist()  # positions\n",
    "        values = sparse_vec[indices].cpu().tolist()  # weights/scores\n",
    "        # build sparse values dictionary\n",
    "        sparse_values = {\n",
    "            \"indices\": indices,\n",
    "            \"values\": values\n",
    "        }\n",
    "        all_data.append(\n",
    "            {\"id\":new_id,\n",
    "             \"values\":dense_embedding,\n",
    "             \"sparse_values\": sparse_values,\n",
    "             \"metadata\":metadata}\n",
    "        )\n",
    "        unembedded_data[new_id] = chunks[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f6092f",
   "metadata": {},
   "source": [
    "## Use Pinecone to host and search snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48886197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "pinecone.init(api_key = api_keys[\"PINECONE_API_KEY\"], environment=\"us-west1-gcp-free\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71483cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"movie-finder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32937cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6fe05e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.create_index(\n",
    "    index_name,\n",
    "    dimension=768,\n",
    "    metric=\"dotproduct\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7287cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pinecone.GRPCIndex(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "663fad8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "def batch_list(input_list, n=100):\n",
    "    for i in range(0, len(input_list), n):\n",
    "        yield input_list[i:i + n]\n",
    "counter = 0\n",
    "for batch in batch_list(all_data):\n",
    "    print(counter)\n",
    "    index.upsert(vectors=batch)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494a6d27",
   "metadata": {},
   "source": [
    "## Semantic search with GPT for reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "7ed73560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = api_keys[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "4e099f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str):\n",
    "    dense_vec = dense_model.encode(text).tolist()\n",
    "    input_ids = tokenizer(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        sparse_vec = sparse_model(\n",
    "            d_kwargs=input_ids.to(device)\n",
    "        )['d_rep'].squeeze()\n",
    "    indices = sparse_vec.nonzero().squeeze().cpu().tolist()\n",
    "    values = sparse_vec[indices].cpu().tolist()\n",
    "    sparse_dict = {\"indices\": indices, \"values\": values}\n",
    "    return dense_vec, sparse_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "81b69860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(ids):\n",
    "    result = []\n",
    "    for id, metadata in ids:\n",
    "        result.append(unembedded_data[id] + metadata)\n",
    "    return \"\\n\".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3ccf4b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metadata(metadata):\n",
    "    return f\"==film_title: {metadata['title']}, genre(s): {metadata['genre_ids']}, original language: {language_codes[metadata['original_language']]}, release_date: {metadata['release_date']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "210cf4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(question, k=5):\n",
    "    dense, sparse = encode(question)\n",
    "    xc = index.query(\n",
    "        vector=dense,\n",
    "        sparse_vector=sparse,\n",
    "        top_k=k, \n",
    "        include_metadata=True\n",
    "    )\n",
    "    response_ids_metadata = [(match[\"id\"], format_metadata(match[\"metadata\"])) for match in xc[\"matches\"]]\n",
    "    response = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo-0613\",\n",
    "      messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"Using only the following context:\\n{get_context(response_ids_metadata)}\\nAnswer this question: {question}\"}\n",
    "        ]\n",
    "    )\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8fda23c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie being referred to in the given context is \"The Departed.\"\n"
     ]
    }
   ],
   "source": [
    "get_response(\"movie with undercover cop and undercover mafia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b2069884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie where two guys save a gym is called \"Dumbbells.\"\n"
     ]
    }
   ],
   "source": [
    "get_response(\"movie where two guys save a gym\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a31407e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no mention of a movie where a monkey goes to the moon in the given context.\n"
     ]
    }
   ],
   "source": [
    "get_response(\"movie where monkey goes to the moon.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "df2ab9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie where a ghost tells a soldier to kill someone with poison is \"The Discreet Charm of the Bourgeoisie\" (release date: 1972-09-15).\n"
     ]
    }
   ],
   "source": [
    "get_response(\"movie where ghost tells a soldier to kill someone with poison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50482d0",
   "metadata": {},
   "source": [
    "# Second Functionality - text2SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62761d80",
   "metadata": {},
   "source": [
    "## Scrape Letterbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb82ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal of this aspect of the project is to enable arbitrary queries of the type:\n",
    "    #I want (genre) (language), (release date), (runtime) films that are similar to x,y, ... in (genre),(description),(cast),(crew)\n",
    "# This involves first scraping all data related to cast, genre, themes, etc. films from letterbox and uploading to a db\n",
    "# then use GPT to convert a natural language query into a SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "dbd881d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_themes(soup):\n",
    "    sections = soup.find_all('section', {'class': 'genre-group'})\n",
    "    section_texts = []\n",
    "    for section in sections:\n",
    "        section_text = section.text.strip()\n",
    "        section_texts.append(section_text)\n",
    "    return section_texts\n",
    "def get_cast(soup):\n",
    "    cast_list_div = soup.find('div', {'class': 'cast-list text-sluglist'})\n",
    "    cast_names = []\n",
    "    if cast_list_div:\n",
    "        actor_tags = cast_list_div.find_all('a', {'class': 'text-slug tooltip'})\n",
    "        for tag in actor_tags:\n",
    "            cast_names.append(tag.text.lower())\n",
    "    return cast_names\n",
    "def get_genres(soup):\n",
    "    genres_div = soup.find('div', {'class': 'text-sluglist capitalize'})\n",
    "    genre_names = []\n",
    "    if genres_div:\n",
    "        genre_tags = genres_div.find_all('a', {'class': 'text-slug'})\n",
    "        for tag in genre_tags:\n",
    "            genre_names.append(tag.text)\n",
    "    return genre_names\n",
    "def get_studios(soup):\n",
    "    h3_studios = soup.find('h3', text=lambda t: 'Studios' in t if t else False)\n",
    "    studio_names = []\n",
    "    if h3_studios:\n",
    "        studios_div = h3_studios.find_next('div', {'class': 'text-sluglist'})\n",
    "        if studios_div:\n",
    "            studio_tags = studios_div.find_all('a', {'class': 'text-slug'})\n",
    "            for tag in studio_tags:\n",
    "                studio_names.append(tag.text)\n",
    "    return studio_names\n",
    "\n",
    "def get_original_language(soup):\n",
    "    h3_language = soup.find('h3', text=lambda t: 'Original Language' in t if t else False)\n",
    "    original_languages = []\n",
    "    if h3_language:\n",
    "        language_div = h3_language.find_next('div', {'class': 'text-sluglist'})\n",
    "\n",
    "        if language_div:\n",
    "            language_tags = language_div.find_all('a', {'class': 'text-slug'})\n",
    "            for tag in language_tags:\n",
    "                original_languages.append(tag.text)\n",
    "    return original_languages[0].lower() if len(original_languages) else \"\"\n",
    "\n",
    "def get_similar(soup):\n",
    "    target_links = []\n",
    "    div_tags = soup.find_all('div', {'class': 'really-lazy-load'})\n",
    "    for div in div_tags:\n",
    "        link = div.get('data-target-link')\n",
    "        if link:\n",
    "            target_links.append(link.replace(\"/film/\", \"\").strip(\"/\"))\n",
    "    return target_links\n",
    "\n",
    "def get_release_year(soup):\n",
    "    year = soup.select_one('section#featured-film-header small.number a').text\n",
    "    return year\n",
    "\n",
    "def get_runtime(soup):\n",
    "    p_tag = soup.find('p', {'class': ['text-link', 'text-footer']})\n",
    "    if p_tag:\n",
    "        text_content = p_tag.text\n",
    "        match = re.search(r'(\\d+)\\s*mins', text_content)\n",
    "        if match:\n",
    "            mins = int(match.group(1))   \n",
    "            return mins\n",
    "        \n",
    "def get_watched_stats(soup):\n",
    "    try:\n",
    "        watched = soup.find('li', {'class': 'js-route-watches'}).find('a', {'class': 'tooltip'})['title'].split()[0].replace(',', '')\n",
    "    except:\n",
    "        watched = 0\n",
    "    try:\n",
    "        fans = soup.find('li', {'class': 'js-route-fans'}).find('a', {'class': 'tooltip'})['title'].split()[0].replace(',', '')\n",
    "    except:\n",
    "        fans = 0\n",
    "    try:\n",
    "        likes = soup.find('li', {'class': 'js-route-likes'}).find('a', {'class': 'tooltip'})['title'].split()[0].replace(',', '')\n",
    "    except:\n",
    "        likes = 0\n",
    "    try:\n",
    "        reviews = soup.find('li', {'class': 'js-route-reviews'}).find('a', {'class': 'tooltip'})['title'].split()[0].replace(',', '')\n",
    "    except:\n",
    "        reviews = 0\n",
    "    return watched, fans, likes, reviews\n",
    "\n",
    "def get_crew(soup):\n",
    "    roles_and_names = {}\n",
    "    for h3_tag in soup.find_all('h3'):\n",
    "        role_span = h3_tag.find('span', {'class': 'crewrole -full'})\n",
    "        if role_span:\n",
    "            role_full = role_span.text\n",
    "            name_div = h3_tag.find_next('div', {'class': 'text-sluglist'})\n",
    "            if name_div:\n",
    "                name_tags = name_div.find_all('a', {'class': 'text-slug'})\n",
    "                names = [name_tag.text.lower() for name_tag in name_tags]\n",
    "                roles_and_names[role_full.lower()] = names\n",
    "    return roles_and_names\n",
    "\n",
    "def modify_movie(movie_title):\n",
    "    return ''.join(char for char in movie_title if char not in string.punctuation).lower().replace(' ', '-')\n",
    "\n",
    "def find_movie(movie, url_base):\n",
    "    movie_title = movie[0]\n",
    "    year = movie[2]\n",
    "    modified_title = modify_movie(movie_title)\n",
    "    with_year = f\"{modified_title}-{year}\"\n",
    "    soup = get_soup(f\"{url_base}{with_year}\")\n",
    "    if \"Not Found\" in soup.title.string:\n",
    "        soup = get_soup(f\"{url_base}{modified_title}\")\n",
    "        if \"Not Found\" in soup.title.string:\n",
    "            return None, None, None\n",
    "        else:\n",
    "            validate_year = get_release_year(soup)\n",
    "            if validate_year == year:\n",
    "                return soup, modified_title, year\n",
    "            else:\n",
    "                return None, None, None\n",
    "    else:\n",
    "        return soup,with_year,year\n",
    "\n",
    "def scrape_all(movie, url_base):\n",
    "    soup, letterbox_title, year = find_movie(movie, url_base)\n",
    "    if soup is None:\n",
    "        return None\n",
    "    scraped_dict = {}\n",
    "    scraped_dict[\"title\"] = movie[0]\n",
    "    scraped_dict[\"genres\"] = get_genres(soup)\n",
    "    scraped_dict[\"studios\"] = get_studios(soup)\n",
    "    scraped_dict[\"original_language\"] = get_original_language(soup)\n",
    "    scraped_dict[\"release year\"] = year\n",
    "    scraped_dict[\"runtime\"] = get_runtime(soup)\n",
    "    crew_dict = get_crew(soup)\n",
    "    crew_dict[\"actor\"] = get_cast(soup)\n",
    "    scraped_dict[\"crew\"] = crew_dict\n",
    "    scraped_dict[\"themes\"] = get_themes(get_soup(f\"{url_base}{letterbox_title}/themes/\"))\n",
    "    all_similar = get_similar(get_soup(f\"{url_base}{letterbox_title}/similar\"))\n",
    "    scraped_dict[\"similar\"] = [film for film in all_similar if film != letterbox_title]\n",
    "    stats = get_watched_stats(get_soup(f\"{url_base}{letterbox_title}/members\"))\n",
    "    scraped_dict[\"num_watched\"] = stats[0]\n",
    "    scraped_dict[\"num_fans\"] = stats[1]\n",
    "    scraped_dict[\"num_liked\"] = stats[2]\n",
    "    scraped_dict[\"num_reviewed\"] = stats[3]\n",
    "    return {letterbox_title: scraped_dict}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = \"https://letterboxd.com/film/\"\n",
    "for movie in tqdm(all_movies[:10]):\n",
    "    full_dict.update(scrape_all(movie, url_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e78420",
   "metadata": {},
   "source": [
    "## Your data from letterbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "0207cd47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Letterboxd URI</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-08-18</td>\n",
       "      <td>The Truman Show</td>\n",
       "      <td>1998</td>\n",
       "      <td>https://boxd.it/18U8</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-08-18</td>\n",
       "      <td>Groundhog Day</td>\n",
       "      <td>1993</td>\n",
       "      <td>https://boxd.it/2b3e</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-08-18</td>\n",
       "      <td>Yes Man</td>\n",
       "      <td>2008</td>\n",
       "      <td>https://boxd.it/1WPW</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-08-18</td>\n",
       "      <td>Bruce Almighty</td>\n",
       "      <td>2003</td>\n",
       "      <td>https://boxd.it/2aCC</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-08-18</td>\n",
       "      <td>Click</td>\n",
       "      <td>2006</td>\n",
       "      <td>https://boxd.it/1YVM</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date             Name  Year        Letterboxd URI  Rating\n",
       "0  2023-08-18  The Truman Show  1998  https://boxd.it/18U8     3.5\n",
       "1  2023-08-18    Groundhog Day  1993  https://boxd.it/2b3e     3.0\n",
       "2  2023-08-18          Yes Man  2008  https://boxd.it/1WPW     3.0\n",
       "3  2023-08-18   Bruce Almighty  2003  https://boxd.it/2aCC     3.0\n",
       "4  2023-08-18            Click  2006  https://boxd.it/1YVM     2.0"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download from https://letterboxd.com/user/exportdata\n",
    "letterbox_csv = \"/Users/sparajuli/Downloads/letterboxd-samp830-2023-08-28-11-36-utc/ratings.csv\"\n",
    "letterbox_df = pd.read_csv(letterbox_csv)\n",
    "letterbox_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1128d986",
   "metadata": {},
   "source": [
    "## Upload data to PostgresSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2632ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "6cfbc1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\", \n",
    "    user=api_keys[\"POSTGRES_USER\"],\n",
    "    password=api_keys[\"POSTGRES_PASSWORD\"],\n",
    "    port=\"5432\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS movies (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    title VARCHAR(255),\n",
    "    letterbox_title VARCHAR(255) UNIQUE,\n",
    "    original_language VARCHAR(50),\n",
    "    release_year INT,\n",
    "    runtime INT,\n",
    "    num_watched INT,\n",
    "    num_fans INT,\n",
    "    num_liked INT,\n",
    "    num_reviewed INT\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS genres (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    genre_name VARCHAR(255) UNIQUE\n",
    ");\n",
    "\"\"\")\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS roles (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    role_name VARCHAR(255) UNIQUE\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS themes (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    theme_name VARCHAR(255) UNIQUE\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS studios (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    studio_name VARCHAR(255) UNIQUE\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS similar_movies (\n",
    "    movie_id INT,\n",
    "    similar_movie_id INT,\n",
    "    PRIMARY KEY (movie_id, similar_movie_id),\n",
    "    FOREIGN KEY (movie_id) REFERENCES movies (id),\n",
    "    FOREIGN KEY (similar_movie_id) REFERENCES movies (id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS movie_crew (\n",
    "    movie_id INT,\n",
    "    role_id INT,\n",
    "    crew_name VARCHAR(255),\n",
    "    PRIMARY KEY (movie_id, role_id, crew_name),\n",
    "    FOREIGN KEY (movie_id) REFERENCES movies (id),\n",
    "    FOREIGN KEY (role_id) REFERENCES roles (id)\n",
    ");\n",
    "\"\"\")\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS movie_studios (\n",
    "    movie_id INT,\n",
    "    studio_id INT,\n",
    "    PRIMARY KEY (movie_id, studio_id),\n",
    "    FOREIGN KEY (movie_id) REFERENCES movies (id),\n",
    "    FOREIGN KEY (studio_id) REFERENCES studios (id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS movie_genres (\n",
    "    movie_id INT,\n",
    "    genre_id INT,\n",
    "    PRIMARY KEY (movie_id, genre_id),\n",
    "    FOREIGN KEY (movie_id) REFERENCES movies (id),\n",
    "    FOREIGN KEY (genre_id) REFERENCES genres (id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS movie_themes (\n",
    "    movie_id INT,\n",
    "    theme_id INT,\n",
    "    PRIMARY KEY (movie_id, theme_id),\n",
    "    FOREIGN KEY (movie_id) REFERENCES movies (id),\n",
    "    FOREIGN KEY (theme_id) REFERENCES themes (id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "dataset = full_dict\n",
    "\n",
    "for key, movie_data in dataset.items():\n",
    "    cur.execute(\"\"\"\n",
    "    INSERT INTO movies (title, letterbox_title, original_language, release_year, runtime, num_watched, num_fans, num_liked, num_reviewed)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING id;\n",
    "    \"\"\", (\n",
    "        movie_data.get('title', None),\n",
    "        key,\n",
    "        movie_data.get('original_language', None),\n",
    "        movie_data.get('release year', None),\n",
    "        movie_data.get('runtime', None),\n",
    "        movie_data.get('num_watched', None),\n",
    "        movie_data.get('num_fans', None),\n",
    "        movie_data.get('num_liked', None),\n",
    "        movie_data.get('num_reviewed', None)\n",
    "    ))\n",
    "\n",
    "    movie_id = cur.fetchone()[0]\n",
    "\n",
    "    for genre in movie_data.get('genres', []):\n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO genres (genre_name) VALUES (%s) ON CONFLICT (genre_name) DO NOTHING;\n",
    "        \"\"\", (genre,))\n",
    "\n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO movie_genres (movie_id, genre_id) SELECT %s, id FROM genres WHERE genre_name = %s;\n",
    "        \"\"\", (movie_id, genre))\n",
    "        \n",
    "    for studio in movie_data.get('studios', []):\n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO studios (studio_name) VALUES (%s) ON CONFLICT (studio_name) DO NOTHING;\n",
    "        \"\"\", (studio,))\n",
    "\n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO movie_studios (movie_id, studio_id) SELECT %s, id FROM studios WHERE studio_name = %s;\n",
    "        \"\"\", (movie_id, studio))\n",
    "        \n",
    "    for theme in movie_data.get('themes', []):\n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO themes (theme_name) VALUES (%s) ON CONFLICT (theme_name) DO NOTHING;\n",
    "        \"\"\", (theme,))\n",
    "\n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO movie_themes (movie_id, theme_id) SELECT %s, id FROM themes WHERE theme_name = %s;\n",
    "        \"\"\", (movie_id, theme))\n",
    "        \n",
    "    for role, crew_names in movie_data.get('crew', {}).items():\n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO roles (role_name) VALUES (%s) ON CONFLICT (role_name) DO NOTHING;\n",
    "        \"\"\", (role,))\n",
    "\n",
    "        for crew_name in crew_names:\n",
    "            cur.execute(\"\"\"\n",
    "            INSERT INTO movie_crew (movie_id, role_id, crew_name)\n",
    "            SELECT %s, id, %s FROM roles WHERE role_name = %s;\n",
    "            \"\"\", (movie_id, crew_name, role))\n",
    "    \n",
    "    for similar_movie in movie_data.get('similar', []):\n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO similar_movies (movie_id, similar_movie_id)\n",
    "        SELECT %s, id FROM movies WHERE letterbox_title = %s;\n",
    "        \"\"\", (movie_id, similar_movie))\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "065f5447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "splade",
   "language": "python",
   "name": "splade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
