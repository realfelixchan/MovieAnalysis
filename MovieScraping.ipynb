{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9493d53-b65a-4913-a1f3-676cbd118a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import airflow\n",
    "\n",
    "# Scraping and dataframe imports\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import Levenshtein\n",
    "import tmdbsimple as tmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e4f906a-050d-4d9a-a037-e47872e7d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will be the first in multiple scripts that are meant to scrape movie data, and feed a ML model\n",
    "## It is being split by functionality in order to make it easier to version and use in orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f8a8783-1d03-48a4-aef1-b4f7c6e8fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## API Key related functions\n",
    "def closest_key(query, dictionary):\n",
    "    closest_key = min(dictionary.keys(), key=lambda k: Levenshtein.distance(query, k))\n",
    "    return dictionary[closest_key]\n",
    "def find_repeated_first_elements(tuples_list):\n",
    "    element_dict = defaultdict(list)\n",
    "    repeated_elements = {}\n",
    "    for t in tuples_list:\n",
    "        element_dict[t[0]].append(t)\n",
    "    \n",
    "    for key, value in element_dict.items():\n",
    "        if len(value) > 1:\n",
    "            repeated_elements[key] = value\n",
    "    \n",
    "    return repeated_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6fc738b-deae-43f4-a218-33a9526057ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scraping related functions\n",
    "def get_soup(url):\n",
    "    response = requests.get(url)\n",
    "    page_content = response.content\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    return soup\n",
    "def get_all_movies(url):\n",
    "    soup = get_soup(url)\n",
    "    movie_info = []\n",
    "\n",
    "    for li in soup.find_all('li'):\n",
    "        a = li.find('a')\n",
    "        if a:\n",
    "            title = a.get('title')\n",
    "            href = a.get('href')\n",
    "\n",
    "            year_match = re.search(r'\\((\\d{4})\\)', li.text)\n",
    "            year = year_match.group(1) if year_match else 'Unknown'\n",
    "            if year == \"Unknown\":\n",
    "                continue\n",
    "            try:\n",
    "                title = re.sub(r' \\([^)]*(film|movie)[^)]*\\)', '', title)\n",
    "            except:\n",
    "                continue\n",
    "            movie_info.append((title, href, year))\n",
    "    \n",
    "    for li in soup.find_all('li'):\n",
    "        main_title = li.find('i').text if li.find('i') else ''\n",
    "\n",
    "        a_tags = li.find_all('a')\n",
    "        for a in a_tags:\n",
    "            title = a.get('title', main_title)\n",
    "            href = a.get('href', '')\n",
    "\n",
    "            year_match = re.search(r'(\\d{4})', a.text)\n",
    "            year = year_match.group(1) if year_match else 'Unknown'\n",
    "            if year == \"Unknown\":\n",
    "                continue\n",
    "            title = re.sub(r' \\([^)]*(film|movie)[^)]*\\)', '', title)\n",
    "            movie_info.append((title, href, year))\n",
    "    return list(set(movie_info))\n",
    "\n",
    "## Getting snippets off of the wikipedia pages\n",
    "def chunk_text(text, n, overlap):\n",
    "    if overlap >= n:\n",
    "        return \"Overlap must be smaller than chunk size\"\n",
    "    length = len(text)\n",
    "    start = 0\n",
    "    chunks = []\n",
    "    while start < length:\n",
    "        end = min(start + n, length)\n",
    "        chunks.append(text[start:end])\n",
    "        start += n - overlap\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "def get_page_content(href):\n",
    "    soup = get_soup(f'https://en.wikipedia.org/{href}')\n",
    "    article_container = soup.find('div', {'id': 'mw-content-text'})\n",
    "    article_text = ''\n",
    "    for paragraph in article_container.find_all('p'):\n",
    "        article_text += paragraph.text\n",
    "    infobox = soup.find('table', {'class': 'infobox'})\n",
    "\n",
    "    infobox_text = \"\"\n",
    "\n",
    "    if infobox:\n",
    "        for row in infobox.find_all('tr'):\n",
    "            # Extract the header if available\n",
    "            th = row.find('th')\n",
    "            if th:\n",
    "                infobox_text += th.get_text() + \"\\n\"\n",
    "\n",
    "            # Extract the data in the row\n",
    "            td = row.find('td')\n",
    "            if td:\n",
    "                infobox_text += td.get_text() + \"\\n\"\n",
    "    article_text += infobox_text\n",
    "    div_tag = soup.find('div', {'class': 'div-col'})\n",
    "    ul_tag = div_tag.find('ul') if div_tag else None\n",
    "    list_text = \"\"\n",
    "    if ul_tag:\n",
    "        for li_tag in ul_tag.find_all('li'):\n",
    "            list_text += li_tag.get_text()\n",
    "    article_text += list_text\n",
    "    return article_text\n",
    "\n",
    "## *** This doesn't seem like it ever gets used? ***\n",
    "def get_chunked_embeddings(href, embedding_model):\n",
    "    page_content = get_page_content(href)\n",
    "    embeddings = embedding_model.encode(chunker(page_content))\n",
    "    return embeddings\n",
    "\n",
    "## TMDB scraping related functions\n",
    "def id2name(id_list):\n",
    "    return [movie_genre_dict[idz] for idz in id_list]\n",
    "\n",
    "def modify_metadata(metadata):\n",
    "    keys_to_delete = []\n",
    "    for key in metadata.keys():\n",
    "        if metadata[key] is None:\n",
    "            keys_to_delete.append(key)\n",
    "\n",
    "    for key in keys_to_delete:\n",
    "        del metadata[key]\n",
    "    metadata[\"genre_ids\"] = id2name(metadata[\"genre_ids\"])\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9630505d-4796-4538-86eb-b75642082922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save a json file with api keys for tmdb, openai, pinecone, and postgres username/password\n",
    "api_keys = json.load(open(\"./Credentials/api_keys.json\"))\n",
    "tmdb.API_KEY = api_keys[\"TMDB_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d1c2cc-cbdd-40d5-ba2c-bf955f9c6e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#language codes tmdb uses\n",
    "language_codes = {\"af\":\"Afrikaans\", \"sq\":\"Albanian\", \"ar\":\"Arabic\", \"eu\": \"Basque\", \"bg\":\"Bulgarian\",\n",
    "                  \"ca\":\"Catalan\", \"cn\":\"Chinese\", \"zh\":\"Chinese\", \"hr\":\"Croatian\", \"cs\":\"Czech\", \"da\": \"Danish\", \"nl\":\"Dutch\",\n",
    "                  \"en\":\"English\", \"et\": \"Estonian\", \"fi\":\"Finnish\", \"fr\":\"French\", \"de\": \"German\", \"el\": \"Greek\",\n",
    "                  \"he\":\"Hebrew\", \"hi\": \"Hindi\", \"hu\":\"Hungarian\", \"is\":\"Icelandic\", \"in\": \"Indonesian\", \"it\": \"Italian\",\n",
    "                  \"ja\": \"Japanese\", \"ko\": \"Korean\", \"lv\": \"Latvian\", \"lt\":\"Lithuanian\", \"mk\": \"Macedonian\", \"ms\": \"malay\",\n",
    "                  \"no\": \"Norwegian\", \"pl\":\"Polish\", \"pt\": \"Portugese\", \"rm\": \"Raeto-Romance\", \"ro\":\"Romanian\", \"ru\":\"Russian\",\n",
    "                  \"sr\": \"Serbian\", \"sl\": \"Slovak\", \"sk\": \"Slovenian\", \"es\": \"Spanish\", \"sv\": \"Swedish\", \"tr\": \"Thai\",\n",
    "                  \"th\": \"Turkish\", \"vi\": \"Vietnamese\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "555305f4-a3d8-436c-9a85-9c34a83852b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We get a list of all movies from wikipedia which serves as a source of information that will be utilized in the semantic\n",
    "#search aspect of this tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ea6f615-7a97-4c98-8601-aff6984406ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_urls = [\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_A\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_B\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_C\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_D\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_E\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_F\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_G\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_H\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_I\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_J%E2%80%93K\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_L\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_M\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_N%E2%80%93O\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_P\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_Q%E2%80%93R\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_S\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_T\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_U%E2%80%93W\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_films:_X%E2%80%93Z\"\n",
    "]\n",
    "all_movies = []\n",
    "for url in all_urls:\n",
    "    all_movies = all_movies + get_all_movies(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35df713f-61df-4eb5-ae24-621dcfe09572",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = tmdb.Search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b66ff5-c311-42d4-9c8f-f3f0b475b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unembedded_data = {}\n",
    "all_data = []\n",
    "for index, movie in tqdm(enumerate(d_subset), total=len(d_subset)):\n",
    "    #How to make a search using TMDB\n",
    "    tmdb_responses = search.movie(query=movie[0])[\"results\"]\n",
    "    if not len(tmdb_responses):\n",
    "        print(f\"{movie[0]} was not found in db\")\n",
    "        continue\n",
    "    metadata = None\n",
    "    for result in tmdb_responses:\n",
    "        if result[\"release_date\"][:4] == movie[2]:\n",
    "            metadata = result\n",
    "    if metadata == None:\n",
    "        continue\n",
    "    metadata = modify_metadata(metadata)\n",
    "    page_content = get_page_content(movie[1])\n",
    "    chunks = chunk_text(page_content, 384, 20)\n",
    "    chunked_embeddings = dense_model.encode(chunks)\n",
    "    input_ids = tokenizer(\n",
    "        chunks, return_tensors='pt',\n",
    "        padding=True, truncation=True\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        sparse_vecs = sparse_model(\n",
    "            d_kwargs=input_ids.to(device)\n",
    "        )['d_rep'].squeeze()\n",
    "    for index, embedding in enumerate(chunked_embeddings):\n",
    "        new_id = f\"{metadata['id']}-{index}\"\n",
    "        dense_embedding = embedding.tolist()\n",
    "        sparse_vec = sparse_vecs[index]\n",
    "        indices = sparse_vec.nonzero().squeeze().cpu().tolist()  # positions\n",
    "        values = sparse_vec[indices].cpu().tolist()  # weights/scores\n",
    "        # build sparse values dictionary\n",
    "        sparse_values = {\n",
    "            \"indices\": indices,\n",
    "            \"values\": values\n",
    "        }\n",
    "        all_data.append(\n",
    "            {\"id\":new_id,\n",
    "             \"values\":dense_embedding,\n",
    "             \"sparse_values\": sparse_values,\n",
    "             \"metadata\":metadata}\n",
    "        )\n",
    "        unembedded_data[new_id] = chunks[index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
